{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing 2 Lab03\n",
    "\n",
    "## Introduction\n",
    "\n",
    "On this part, we will evaluate a simple semantic search with and without nearest neighbours approximation. We fill first create a searchable index, then evaluate queries on it in terms of Mean Average Precision and speed.\n",
    "\n",
    "### Create a searchable index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unikarah/.local/lib/python3.10/site-packages/beir/util.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03382158279418945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4635922,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e54677217b4a61835100dc600bd480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4635922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the Beir library to extract the test set of the DBPedia entity dataset.\n",
    "\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "def extract_test(dataset):\n",
    "    \"\"\"\n",
    "    Extract the test test set with the Beir library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        The name of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corpus, queries, qrels : dict\n",
    "        The test set.\n",
    "    \"\"\"\n",
    "    url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "    data_path = util.download_and_unzip(url, \"datasets\")\n",
    "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    return corpus, queries, qrels\n",
    "\n",
    "dataset = \"dbpedia-entity\"\n",
    "corpus, queries, qrels = extract_test(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the three values returned by Beir, and how are they presented ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's see what the first values of each value look like*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INEX_LD-2009022', 'Szechwan dish food cuisine'),\n",
       " ('INEX_LD-2009039', 'roman architecture'),\n",
       " ('INEX_LD-2009053', 'finland car industry manufacturer saab sisu'),\n",
       " ('INEX_LD-2009061', 'france second world war normandy'),\n",
       " ('INEX_LD-2009062', 'social network group selection'),\n",
       " ('INEX_LD-2009063', 'D-Day normandy invasion'),\n",
       " ('INEX_LD-2009074', 'web ranking scoring algorithm'),\n",
       " ('INEX_LD-2009115', 'virtual museums'),\n",
       " ('INEX_LD-2010004', 'Indian food'),\n",
       " ('INEX_LD-2010014', 'composer museum')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(queries.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<dbpedia:Animalia_(book)>',\n",
       "  {'text': \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold.   A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\",\n",
       "   'title': 'Animalia (book)'}),\n",
       " ('<dbpedia:Academy_Award_for_Best_Production_Design>',\n",
       "  {'text': \"The Academy Awards are the oldest awards ceremony for achievements in motion pictures. The Academy Award for Best Production Design recognizes achievement in art direction on a film. The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards.  This change resulted from the Art Director's branch of the Academy being renamed the Designer's branch.\",\n",
       "   'title': 'Academy Award for Best Production Design'}),\n",
       " ('<dbpedia:An_American_in_Paris>',\n",
       "  {'text': 'An American in Paris is a jazz-influenced symphonic poem by the American composer George Gershwin, written in 1928. Inspired by the time Gershwin had spent in Paris, it evokes the sights and energy of the French capital in the 1920s and is one of his best-known compositions.Gershwin composed An American in Paris on commission from the conductor Walter Damrosch. He scored the piece for the standard instruments of the symphony orchestra plus celesta, saxophones, and automobile horns.',\n",
       "   'title': 'An American in Paris'}),\n",
       " ('<dbpedia:Astronomer>',\n",
       "  {'text': 'An astronomer is a scientist in the field of astronomy who studies stars, planets, moons, comets, and galaxies, as well as many other celestial objects. A related but distinct subject, cosmology, is concerned with studying the universe as a whole. An astronomer researches the world beyond Earth.',\n",
       "   'title': 'Astronomer'}),\n",
       " ('<dbpedia:Answer>',\n",
       "  {'text': 'Generally, an answer is a reply to a question.',\n",
       "   'title': 'Answer'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(corpus.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<dbpedia:Afghan_cuisine>': 0,\n",
       " '<dbpedia:Akan_cuisine>': 0,\n",
       " '<dbpedia:Ambuyat>': 0,\n",
       " '<dbpedia:American_Chinese_cuisine>': 1,\n",
       " '<dbpedia:Ants_climbing_a_tree>': 2,\n",
       " '<dbpedia:Baingan_bharta>': 1,\n",
       " '<dbpedia:Bamischijf>': 0,\n",
       " '<dbpedia:Black_cardamom>': 0,\n",
       " '<dbpedia:Brazilian_cuisine>': 0,\n",
       " '<dbpedia:British_cuisine>': 0,\n",
       " '<dbpedia:Caribbean_cuisine>': 0,\n",
       " '<dbpedia:Cellophane_noodles>': 1,\n",
       " '<dbpedia:Ceviche>': 0,\n",
       " '<dbpedia:Chana_masala>': 0,\n",
       " '<dbpedia:Chen_Kenichi>': 1,\n",
       " '<dbpedia:Chen_Kenmin>': 1,\n",
       " '<dbpedia:Chicago-style_pizza>': 0,\n",
       " '<dbpedia:Chicken_(food)>': 0,\n",
       " '<dbpedia:Chifle>': 0,\n",
       " '<dbpedia:Chili_oil>': 2,\n",
       " '<dbpedia:Chinatown,_Los_Angeles>': 0,\n",
       " '<dbpedia:Chinatown>': 1,\n",
       " '<dbpedia:Chinese_cuisine>': 2,\n",
       " '<dbpedia:Churumuri_(food)>': 0,\n",
       " '<dbpedia:Cookbook>': 0,\n",
       " '<dbpedia:Cooking>': 0,\n",
       " '<dbpedia:Couscous>': 0,\n",
       " '<dbpedia:Cuban_cuisine>': 0,\n",
       " '<dbpedia:Cuisine>': 0,\n",
       " '<dbpedia:Cuisine_of_Jharkhand>': 0,\n",
       " '<dbpedia:Cuisine_of_the_Southern_United_States>': 0,\n",
       " '<dbpedia:Cuisine_of_the_United_States>': 0,\n",
       " '<dbpedia:Culture_of_the_Song_dynasty>': 0,\n",
       " '<dbpedia:Curry>': 0,\n",
       " '<dbpedia:Dal_dhokli>': 0,\n",
       " '<dbpedia:Ding_Baozhen>': 0,\n",
       " '<dbpedia:Dish_(food)>': 0,\n",
       " '<dbpedia:Doubanjiang>': 1,\n",
       " '<dbpedia:Drunken_shrimp>': 0,\n",
       " '<dbpedia:Escabeche>': 0,\n",
       " '<dbpedia:Fermentation_in_food_processing>': 0,\n",
       " '<dbpedia:Food_presentation>': 0,\n",
       " '<dbpedia:Fried_rice>': 0,\n",
       " '<dbpedia:Fuchsia_Dunlop>': 1,\n",
       " '<dbpedia:Fufu>': 0,\n",
       " '<dbpedia:Fuqi_feipian>': 2,\n",
       " '<dbpedia:Gastronomy>': 0,\n",
       " \"<dbpedia:General_Tso's_chicken>\": 0,\n",
       " '<dbpedia:German_cuisine>': 0,\n",
       " '<dbpedia:Ghanaian_cuisine>': 0,\n",
       " '<dbpedia:Global_cuisine>': 0,\n",
       " '<dbpedia:Gondi_dumpling>': 0,\n",
       " '<dbpedia:Greek_cuisine>': 0,\n",
       " '<dbpedia:Guizhou_cuisine>': 1,\n",
       " '<dbpedia:Guoba>': 2,\n",
       " '<dbpedia:Ham_salad>': 0,\n",
       " '<dbpedia:Harees>': 0,\n",
       " '<dbpedia:Hazaragi_cuisine>': 0,\n",
       " '<dbpedia:History_of_Chinese_cuisine>': 1,\n",
       " '<dbpedia:Hong_Kong_cuisine>': 0,\n",
       " '<dbpedia:Hot_and_sour_soup>': 1,\n",
       " '<dbpedia:Hot_pot>': 2,\n",
       " '<dbpedia:Hot_sauce>': 1,\n",
       " '<dbpedia:Huaiyang_cuisine>': 0,\n",
       " '<dbpedia:Hunan_cuisine>': 0,\n",
       " '<dbpedia:Indian_Chinese_cuisine>': 1,\n",
       " '<dbpedia:Indian_Singaporean_cuisine>': 0,\n",
       " '<dbpedia:Indian_cuisine>': 0,\n",
       " '<dbpedia:Indonesian_cuisine>': 0,\n",
       " '<dbpedia:Italian_cuisine>': 0,\n",
       " '<dbpedia:Kebab>': 0,\n",
       " '<dbpedia:Khmer_(food)>': 0,\n",
       " '<dbpedia:Knieperkohl>': 0,\n",
       " '<dbpedia:Korean_cuisine>': 0,\n",
       " '<dbpedia:Korean_royal_court_cuisine>': 0,\n",
       " '<dbpedia:Kung_Pao_chicken>': 1,\n",
       " '<dbpedia:Latin_American_cuisine>': 0,\n",
       " '<dbpedia:Lechon>': 0,\n",
       " '<dbpedia:List_of_Asian_cuisines>': 0,\n",
       " '<dbpedia:List_of_Chinese_dishes>': 1,\n",
       " '<dbpedia:List_of_cuisines>': 0,\n",
       " '<dbpedia:List_of_egg_dishes>': 0,\n",
       " '<dbpedia:List_of_foods>': 0,\n",
       " '<dbpedia:List_of_potato_dishes>': 0,\n",
       " '<dbpedia:Mala_sauce>': 2,\n",
       " '<dbpedia:Malay_cuisine>': 0,\n",
       " '<dbpedia:MangÃº>': 0,\n",
       " '<dbpedia:Mapo_doufu>': 2,\n",
       " '<dbpedia:Mexican_cuisine>': 0,\n",
       " '<dbpedia:National_dish>': 0,\n",
       " '<dbpedia:Omelette>': 0,\n",
       " '<dbpedia:Osh_(food)>': 0,\n",
       " '<dbpedia:Outline_of_cuisines>': 0,\n",
       " '<dbpedia:Outline_of_food_preparation>': 0,\n",
       " '<dbpedia:Pao_cai>': 1,\n",
       " '<dbpedia:Pasta>': 0,\n",
       " '<dbpedia:Philippine_cuisine>': 0,\n",
       " '<dbpedia:Pichanga_(dish)>': 0,\n",
       " '<dbpedia:Pilaf>': 0,\n",
       " '<dbpedia:Porridge>': 0,\n",
       " '<dbpedia:Regional_cuisine>': 0,\n",
       " '<dbpedia:Roast_beef>': 0,\n",
       " '<dbpedia:Run_down>': 0,\n",
       " '<dbpedia:Sanna_(dish)>': 0,\n",
       " '<dbpedia:Seafood_dishes>': 0,\n",
       " '<dbpedia:Shuizhu>': 2,\n",
       " '<dbpedia:Shun_Lee_Palace>': 0,\n",
       " '<dbpedia:Sichuan>': 1,\n",
       " '<dbpedia:Sichuan_cuisine>': 1,\n",
       " '<dbpedia:Sichuan_pepper>': 1,\n",
       " '<dbpedia:Side_dish>': 0,\n",
       " '<dbpedia:Sinki_(food)>': 1,\n",
       " '<dbpedia:Staple_food>': 0,\n",
       " '<dbpedia:Street_food>': 0,\n",
       " '<dbpedia:Suanla_chaoshou>': 2,\n",
       " '<dbpedia:Sundanese_cuisine>': 0,\n",
       " '<dbpedia:Tahini>': 0,\n",
       " '<dbpedia:Taiwanese_cuisine>': 0,\n",
       " '<dbpedia:Thai_cuisine>': 0,\n",
       " '<dbpedia:Tongue>': 0,\n",
       " '<dbpedia:Traditional_food>': 0,\n",
       " '<dbpedia:Turkish_cuisine>': 0,\n",
       " '<dbpedia:Twelve-dish_Christmas_Eve_supper>': 0,\n",
       " '<dbpedia:Twice_cooked_pork>': 2,\n",
       " '<dbpedia:Urap>': 0,\n",
       " '<dbpedia:Vicia_faba>': 1,\n",
       " '<dbpedia:Wonton>': 1,\n",
       " '<dbpedia:Zha_cai>': 1,\n",
       " '<dbpedia:Zhal>': 0,\n",
       " '<dbpedia:Zhangcha_duck>': 1,\n",
       " '<dbpedia:Zigong>': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels['INEX_LD-2009022']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the problem, extract all the document from the corpus which are relevant to at least one query. Then, add 100K random documents which are not relevant to any query. Make sure the process is reproducible by setting the random seed on whatever random sampling method you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the documents chosen: 114877\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "corpus_reduced_dict = {}\n",
    "\n",
    "\n",
    "qrels_irrelevant = []\n",
    "# first we get all the corpus that are relevant to queries\n",
    "for id, query in queries.items():\n",
    "    # Get al the relevant content with the id of the query\n",
    "    qrels_for_id = qrels[id]\n",
    "\n",
    "    # Get all the queries that are relevant (value 1 or 2)\n",
    "    qrels_relevant = []\n",
    "    for id, relevancy in qrels_for_id.items():\n",
    "        if relevancy > 0:\n",
    "            qrels_relevant.append(id)\n",
    "    \n",
    "    # get the corpus elements corresponding and add them the the reduced corpus\n",
    "    for id in qrels_relevant:\n",
    "        corpus_reduced_dict[id] = corpus[id]\n",
    "\n",
    "corpus_irrelevant = {}\n",
    "for id, corp in corpus.items():\n",
    "    if id not in corpus_reduced_dict:\n",
    "        corpus_irrelevant[id] = corp\n",
    "\n",
    "corpus_reduced = list(corpus_reduced_dict.values())\n",
    "\n",
    "# Choose the 100k random documents\n",
    "random.seed(42)\n",
    "corpus_random_keys = random.sample(list(corpus_irrelevant), 100000)\n",
    "\n",
    "for id in corpus_random_keys:\n",
    "    corpus_reduced.append(corpus[id])\n",
    "print(\"Total length of the documents chosen:\", len(corpus_reduced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should be ready to start experimenting with our smaller dataset. Use the\n",
    "sentence-transformers library to index your dataset. As queries and documents\n",
    "are different, use an asymetric similarity models. Pick one model across the\n",
    "ones proposed. Make sure to document your choice, and why you picked it (because\n",
    "of accuracy, speed, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**\n",
    "On the documentation for asymetric similarity models, we can choose from\n",
    "multiple models. Ones being for cosine-similarity and the others being tuned for\n",
    "dot-product. The two models in each categorie having the best accuracies are:\n",
    "- msmarco-distilbert-base-v4 (for cosine-similarity).\n",
    "- msmarco-distilbert-base-tas-b (for dot-product).\n",
    "\n",
    "Here we have short passages, and no very long paragraph. As recommended in the\n",
    "docummentation we are going to use the cosine-similarity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('msmarco-distilbert-base-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the reduced corpus and the queries using the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/unikarah/Documents/NLP_Deep/3_Semantic_Search/3_Semantic_search.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/unikarah/Documents/NLP_Deep/3_Semantic_Search/3_Semantic_search.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m corpus_embedings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(corpus_reduced)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    568\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    569\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    570\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    574\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:345\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    343\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 345\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    346\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:283\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    284\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    285\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    286\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    287\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    288\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    289\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    292\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:225\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(weights, v)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m context \u001b[39m=\u001b[39m unshape(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_lin(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m (context, weights)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_embedings = model.encode(corpus_reduced) # FIXME : run meeeeeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12601008  0.4351175  -0.14884456 ...  0.12693802  0.21826428\n",
      "  -0.12447575]\n",
      " [ 1.3706691   0.35628912 -0.0987019  ...  0.01148714 -0.03544339\n",
      "   0.69143116]\n",
      " [-0.04835236  0.26421046 -0.40233096 ... -0.57687294 -0.16397607\n",
      "  -0.5554676 ]\n",
      " ...\n",
      " [-0.36190474 -0.28840816  0.41469216 ... -0.46373257  0.4913169\n",
      "   0.55152303]\n",
      " [ 0.637174    0.41584215 -0.74660397 ...  0.6245678   1.1596165\n",
      "  -0.18587023]\n",
      " [-0.02936348 -0.07553382  0.3569843  ...  0.553844    0.2052955\n",
      "  -0.7003371 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries_embedings = model.encode(list(queries.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the annotated set of queries, compute the Mean Average Precision (MAP) @100 as well as the average time per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's build the confusion matrix with the result and qrels\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a good set of parameters for the chosen ANN library and compute the Mean Average Precision @100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use the Faiss library for the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# FIXME ALL OF IT\n",
    "xb = corpus\n",
    "xq = queries\n",
    "queries_list = list(queries.values())\n",
    "d = # dimension\n",
    "\n",
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "index.add(corpus_reduced)                  # add vectors to the index\n",
    "\n",
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(corpus_reduced[:5], k) # sanity check\n",
    "print(I)\n",
    "print(D)\n",
    "D, I = index.search(queries_list, k)     # actual search\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print(I[-5:])                  # neighbors of the 5 last queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain what the parameters you picked are, and why you chose them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
